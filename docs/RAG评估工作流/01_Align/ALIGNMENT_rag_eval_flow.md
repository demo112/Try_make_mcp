# 项目对齐文档 (Alignment Document)

## 1. 项目背景
用户需要将一套 RAG (Retrieval-Augmented Generation) 评估流程固化为 MCP 工具。该流程主要用于评估知识库质量、RAG 系统回答准确性以及自动化评分。

## 2. 目标用户
*   **知识库维护者**: 需要验证知识库内容的完整性和准确性。
*   **RAG 系统开发者**: 需要评估 RAG 系统的检索和生成效果。
*   **QA 测试人员**: 需要自动化生成测试用例并进行回归测试。

## 3. 核心能力 (Core Capabilities)
本项目将提供以下核心 MCP 工具：

1.  **生成测试数据集 (`generate_test_dataset`)**:
    *   基于给定的知识库文档（Markdown/Text），利用 LLM 自动生成 "问题-标准答案" 对。
    *   输出格式为 CSV。
2.  **执行问答测试 (`run_qa_test`)**:
    *   读取测试问题。
    *   基于知识库（模拟 RAG 或直接上下文）生成回答。
    *   记录回答结果。
3.  **自动化评分 (`evaluate_answers`)**:
    *   将 "生成回答" 与 "标准答案" 进行比对。
    *   利用 LLM 进行 0-10 分的打分，并给出评分理由。
    *   输出包含评分的完整 CSV。

## 4. 依赖系统
*   **LLM Provider**: 需要调用 Gemini/OpenAI 等大模型进行生成和评估 (使用 `src.common.llm` 或 `fastmcp` 内置能力)。
*   **文件系统**: 读取知识库文件，读写 CSV 文件。
*   **向量数据库 (可选)**: 如果需要模拟真实 RAG，可能需要集成简单的向量检索（如 `chromadb` 或 `faiss`），或者为简化起见，对于小规模文档直接使用全文上下文。
    *   *决策*: 鉴于 MCP 的轻量级特性，初期版本支持 "全文上下文" (适合单文档或小知识库) 和 "简单关键词/向量检索" (如果集成了 `rag_flow` 的逻辑)。
    *   *调整*: 为了保持独立性，本 MCP 将内置轻量级的检索逻辑（或直接引用 `src.common` 中的 RAG 模块如果存在）。

## 5. 约束条件
*   必须遵循 6A 工作流。
*   使用 Python `fastmcp` 框架。
*   输出结果必须可量化、可追溯。
